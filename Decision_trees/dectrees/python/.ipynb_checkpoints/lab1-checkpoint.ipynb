{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0:\n",
    "\n",
    "### Each one of the datasets has properties which makes them hard to learn. Motivate which of the three problems is most difficult for a decision tree algorithm to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that MONK-2 is the hardest to learn. This because it is hard to pick a root node that gives a lot of additional information on the problem. Even if A5 is 0, there has to be three more 0 valued attributes and two 1 valued attributes. Basically, the resulting tree should be 6 nodes deep, since every attribute has to be checked, and the amount of \"True\" combinations that has to be represented within it are 6 choose 2, i.e 15. But I don't think the simplicity of the decision tree algorithm will be able to learn this problem perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: \n",
    "\n",
    "### The file dtree.py defines a function entropy which calculates the entropy of a dataset. Import this file along with the monks datasets and use it to calculate the entropy of the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dtree as d\n",
    "import monkdata as m\n",
    "\n",
    "train = [m.monk1, m.monk2, m.monk3]\n",
    "test = [m.monk1test, m.monk2test, m.monk3test]\n",
    "\n",
    "print(\"monk1\\t\"+\"monk2\\t\\t\\t\"+\"monk3\")\n",
    "print(d.entropy(m.monk1), end='\\t')\n",
    "print(d.entropy(m.monk2), end='\\t')\n",
    "print(d.entropy(m.monk3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "### Explain entropy for a uniform distribution and a non-uniform distribution, present some example distributions with high and low entropy.\n",
    "\n",
    "If all datapoints in set 𝐴 are picked with equal probability 1/𝑚 (𝑚 being cardinality of set 𝐴), then the randomness or the entropy increases. But if we know that some points in set 𝐴 are going to occur with more probability than others (say in the case of normal distribution, where the maximum concentration of data points is around the mean and small standard deviation area around it, then the randomness or entropy should decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "### Use the function averageGain (defined in dtree.py) to calculate the expected information gain corresponding to each of the six attributes. Note that the attributes are represented as in- stances of the class Attribute (defined in monkdata.py) which you can access via m.attributes[0], ..., m.attributes[5]. Based on the results, which attribute should be used for splitting the examples at the root node?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset\ta1\ta2\ta3\ta4\ta5\ta6\n",
      "MONK-1\t0.07527\t0.00584\t0.00471\t0.02631\t0.28703\t0.00076\t\n",
      "MONK-2\t0.00376\t0.00246\t0.00106\t0.01566\t0.01728\t0.00625\t\n",
      "MONK-3\t0.00712\t0.29374\t0.00083\t0.00289\t0.25591\t0.00708\t"
     ]
    }
   ],
   "source": [
    "data_sets = [m.monk1, m.monk2, m.monk3]\n",
    "attributes = m.attributes\n",
    "print(\"Dataset\\ta1\\ta2\\ta3\\ta4\\ta5\\ta6\")\n",
    "i = 1\n",
    "for data_set in data_sets:\n",
    "    if i > 1:\n",
    "        print()\n",
    "    print(\"MONK-\"+str(i), end=\"\\t\")\n",
    "    i +=1\n",
    "    for attribute in attributes:\n",
    "        print(\"%.5f\" % d.averageGain(data_set, attribute), end=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, a5 should be used for splitting the examples at root for monk1 and monk2, and a2 should be used for monk3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "### For splitting we choose the attribute that maximizes the information gain, Eq.3. Looking at Eq.3 how does the entropy of the subsets, Sk, look like when the information gain is maximized? How can we motivate using the information gain as a heuristic for picking an attribute for splitting? Think about reduction in entropy after the split and what the entropy implies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When entropy is low in S_k, the gain is greater, thus leading to a more informed tree. That motivates us to use information gain as a heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5\n",
    "### Build the full decision trees for all three Monk datasets using buildTree. Then, use the function check to measure the performance of the decision tree on both the training and test datasets. Compute the train and test set errors for the three Monk datasets for the full trees. Were your assumptions about the datasets correct? Explain the results you get for the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tE_train\tE_test\n",
      "MONK-1\t0.00000\t0.17130\n",
      "MONK-2\t0.00000\t0.30787\n",
      "MONK-3\t0.00000\t0.05556\n"
     ]
    }
   ],
   "source": [
    "t1 = d.buildTree(m.monk1, m.attributes);\n",
    "t2 = d.buildTree(m.monk2, m.attributes);\n",
    "t3 = d.buildTree(m.monk3, m.attributes);\n",
    "\n",
    "print(\"\\tE_train\\tE_test\")\n",
    "print(\"MONK-1\\t\"+\"%.5f\" % (1-d.check(t1, m.monk1))+ \"\\t%.5f\" % (1-d.check(t1, m.monk1test)))\n",
    "print(\"MONK-2\\t\"+\"%.5f\" % (1-d.check(t2, m.monk2))+ \"\\t%.5f\" % (1-d.check(t2, m.monk2test)))\n",
    "print(\"MONK-3\\t\"+\"%.5f\" % (1-d.check(t3, m.monk3))+ \"\\t%.5f\" % (1-d.check(t3, m.monk3test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The E_train is as expected, the tree has been trained on the data and is now a perfect representation of it, hence E_train = 0. It is also as I suspected, the monk2 dataset is the hardest to model. What is surprising for me however, is that monk1 is harder to model than monk3. Not only does monk3 have noise in the training data, but at first glance I perceived the monk1 logics as simpler than monk3. but maybe the a1=a2 from monk1 adds a lot of complexity to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6\n",
    "### Explain pruning from a bias variance trade-off perspective.\n",
    "\n",
    "When we prune a decision tree we remove sections of the tree that provides little additional information (with respect to the rest of the tree). This reduces variance but increases the bias of the tree. Since decision trees and many other non-parametric models are low bias high variance, pruning is a good way to find the \"sweet spot\" where bias and variance are reasonably low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 7\n",
    "### Evaluate the effect pruning has on the test error for the monk1 and monk3 datasets, in particular determine the optimal partition into training and pruning by optimizing the parameter fraction. Plot the classification error on the test sets as a function of the parameter fraction ∈ {0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. Note that the split of the data is random. We therefore need to compute the statistics over several runs of the split to be able to draw any conclusions. Reasonable statistics includes mean and a measure of the spread. Do remember to print axes labels, legends and data points as you will not pass without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_trees = d.allPruned(t1)\n",
    "fraction = [.3, .4, .5, .6, .7, .8]\n",
    "for tree in pruned_trees:\n",
    "    print(d.check(tree, m.monk1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
